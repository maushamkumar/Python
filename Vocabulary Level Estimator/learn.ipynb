{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85d08cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "df = pd.read_csv('sentiment_tweets3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a73852a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index</th>\n",
       "      <th>message to examine</th>\n",
       "      <th>label (depression result)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>106</td>\n",
       "      <td>just had a real good moment. i missssssssss hi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>217</td>\n",
       "      <td>is reading manga  http://plurk.com/p/mzp1e</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>220</td>\n",
       "      <td>@comeagainjen http://twitpic.com/2y2lx - http:...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>288</td>\n",
       "      <td>@lapcat Need to send 'em to my accountant tomo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>540</td>\n",
       "      <td>ADD ME ON MYSPACE!!!  myspace.com/LookThunder</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Index                                 message to examine  \\\n",
       "0    106  just had a real good moment. i missssssssss hi...   \n",
       "1    217         is reading manga  http://plurk.com/p/mzp1e   \n",
       "2    220  @comeagainjen http://twitpic.com/2y2lx - http:...   \n",
       "3    288  @lapcat Need to send 'em to my accountant tomo...   \n",
       "4    540      ADD ME ON MYSPACE!!!  myspace.com/LookThunder   \n",
       "\n",
       "   label (depression result)  \n",
       "0                          0  \n",
       "1                          0  \n",
       "2                          0  \n",
       "3                          0  \n",
       "4                          0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc6c087",
   "metadata": {},
   "source": [
    "## 1. LowerCase Text \n",
    "\n",
    "Lowercasing text in NLP preprocessing involves converting all letters in a text to lowercase. This step is essential for standardizing text data because it treats words with different cases (e.g., \"Word\" and \"word\") as the same, reducing vocabulary size and improving model efficiency. It ensures consistency in word representations, making it easier for algorithms to recognize patterns and associations. For example, \"The\" and \"the\" are treated as identical after lowercasing. This normalization simplifies subsequent processing steps, such as tokenization and feature extraction, leading to more accurate and robust NLP models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "655a1ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.rename(columns={'message to examine':'review', 'label (depression result)': 'label'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a86003b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"@lapcat Need to send 'em to my accountant tomorrow. Oddly, I wasn't even referring to my taxes. Those are supporting evidence, though. \""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pick a random review \n",
    "df['review'][3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358f0e9e",
   "metadata": {},
   "source": [
    "**if we have a Single Text or Sentence we can Lowercase it by using lower() func of Python.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9a70828d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"@lapcat need to send 'em to my accountant tomorrow. oddly, i wasn't even referring to my taxes. those are supporting evidence, though. \""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lower Casing the review \n",
    "df['review'][3].lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4147cd0",
   "metadata": {},
   "source": [
    "**We can also Lowercase the Whole Corpus by using lower() function of Python.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d9c8621a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index</th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>106</td>\n",
       "      <td>just had a real good moment. i missssssssss hi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>217</td>\n",
       "      <td>is reading manga  http://plurk.com/p/mzp1e</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>220</td>\n",
       "      <td>@comeagainjen http://twitpic.com/2y2lx - http:...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>288</td>\n",
       "      <td>@lapcat need to send 'em to my accountant tomo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>540</td>\n",
       "      <td>add me on myspace!!!  myspace.com/lookthunder</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Index                                             review  label\n",
       "0    106  just had a real good moment. i missssssssss hi...      0\n",
       "1    217         is reading manga  http://plurk.com/p/mzp1e      0\n",
       "2    220  @comeagainjen http://twitpic.com/2y2lx - http:...      0\n",
       "3    288  @lapcat need to send 'em to my accountant tomo...      0\n",
       "4    540      add me on myspace!!!  myspace.com/lookthunder      0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['review'] = df['review'].str.lower()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd7a658",
   "metadata": {},
   "source": [
    "**Now we see all the sentences in the corpus are in lowercase.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209434d7",
   "metadata": {},
   "source": [
    "## 2. Remove HTML Tags\n",
    "\n",
    "¬∂\n",
    "Removing HTML tags is an essential step in NLP text preprocessing to ensure that only meaningful textual content is analyzed. HTML tags contain formatting information and metadata irrelevant to linguistic analysis. Including these tags can introduce noise and distort the analysis results. Removing HTML tags helps to extract pure textual data, making it easier to focus on the actual content of the text. This step is particularly crucial when dealing with web data or documents containing HTML markup, as it ensures that the extracted text accurately represents the intended linguistic information for NLP tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abccff36",
   "metadata": {},
   "source": [
    "**We can simply remove HTML tags by using the Regular Expressions.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "761983ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Regular Expression \n",
    "import re \n",
    "\n",
    "def remove_html_tags(text):\n",
    "    pattern = re.compile('<.*?>')\n",
    "    return pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f00b1cd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<html><body><p> Movie 1</p><p> Actor - Aamir Khan</p><p> Click here to <a href='http://google.com'>download</a></p></body></html>\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Suppose we have a text Which Contains HTML Tags \n",
    "text = \"<html><body><p> Movie 1</p><p> Actor - Aamir Khan</p><p> Click here to <a href='http://google.com'>download</a></p></body></html>\"\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "27d2424f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Movie 1 Actor - Aamir Khan Click here to download'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply Function to remove the HTML Tags \n",
    "remove_html_tags(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4ad890",
   "metadata": {},
   "source": [
    "**See How the Code perform well and clean the text from the HTML Tags , We can Also Apply this Function to Whole Corpus.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5be23045",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index</th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>106</td>\n",
       "      <td>just had a real good moment. i missssssssss hi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>217</td>\n",
       "      <td>is reading manga  http://plurk.com/p/mzp1e</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>220</td>\n",
       "      <td>@comeagainjen http://twitpic.com/2y2lx - http:...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>288</td>\n",
       "      <td>@lapcat need to send 'em to my accountant tomo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>540</td>\n",
       "      <td>add me on myspace!!!  myspace.com/lookthunder</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Index                                             review  label\n",
       "0    106  just had a real good moment. i missssssssss hi...      0\n",
       "1    217         is reading manga  http://plurk.com/p/mzp1e      0\n",
       "2    220  @comeagainjen http://twitpic.com/2y2lx - http:...      0\n",
       "3    288  @lapcat need to send 'em to my accountant tomo...      0\n",
       "4    540      add me on myspace!!!  myspace.com/lookthunder      0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['review'] = df['review'].apply(remove_html_tags)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d76657f",
   "metadata": {},
   "source": [
    "## 3. Remove URLs\n",
    "\n",
    "In NLP text preprocessing, removing URLs is essential to eliminate irrelevant information that doesn't contribute to linguistic analysis. URLs contain website addresses, hyperlinks, and other web-specific elements that can skew the analysis and confuse machine learning models. By removing URLs, the focus remains on the textual content relevant to the task at hand, enhancing the accuracy of NLP tasks such as sentiment analysis, text classification, and information extraction. This step streamlines the dataset, reduces noise, and ensures that the model's attention is directed towards meaningful linguistic patterns and structures within the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2422150e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here We also Use Regular Expressions to Remove URLs from Text or Whole Corpus.\n",
    "def remove_url(text):\n",
    "    pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8243c692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose we have the FOllowings Text With URL.\n",
    "text1 = 'Check out my notebook https://www.kaggle.com/campusx/notebook8223fc1abb'\n",
    "text2 = 'Check out my notebook http://www.kaggle.com/campusx/notebook8223fc1abb'\n",
    "text3 = 'Google search here www.google.com'\n",
    "text4 = 'For notebook click https://www.kaggle.com/campusx/notebook8223fc1abb to search check www.google.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2b011f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check out my notebook \n",
      "Check out my notebook \n",
      "Google search here \n",
      "For notebook click  to search check \n"
     ]
    }
   ],
   "source": [
    "# Lets Remove The URL by Calling Function\n",
    "print(remove_url(text1))\n",
    "print(remove_url(text2))\n",
    "print(remove_url(text3))\n",
    "print(remove_url(text4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac48680",
   "metadata": {},
   "source": [
    "**Here How the function beatuifully remove the URLs from the Text . We Can Simply Call this Function on Whole Corpus to Remove URLs.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb55b59d",
   "metadata": {},
   "source": [
    "## 4. Remove Punctuations\n",
    "\n",
    "¬∂\n",
    "Removing punctuation marks is essential in NLP text preprocessing to enhance the accuracy and efficiency of analysis. Punctuation marks like commas, periods, and quotation marks carry little semantic meaning and can introduce noise into the dataset. By removing them, the text becomes cleaner and more uniform, making it easier for machine learning models to extract meaningful features and patterns. Additionally, removing punctuation aids in standardizing the text, ensuring consistency across documents and improving the overall performance of NLP tasks such as sentiment analysis, text classification, and named entity recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "894f62e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d8833c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing Punctuation in a Variable\n",
    "punc = string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2a077da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code defines a function, remove_punc1, that takes a text input and removes all punctuation characters from it using\n",
    "# the translate method with a translation table created by str.maketrans. This function effectively cleanses the text of punctuation symbols.\n",
    "def remove_punc(text):\n",
    "    return text.translate(str.maketrans('', '', punc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c3b2238b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The quick brown fox jumps over the lazy dog. However, the dog doesn't seem impressed! Oh no, it just yawned. How disappointing! Maybe a squirrel would elicit a reaction. Alas, the fox is out of luck.\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Text With Punctuation.\n",
    "text = \"The quick brown fox jumps over the lazy dog. However, the dog doesn't seem impressed! Oh no, it just yawned. How disappointing! Maybe a squirrel would elicit a reaction. Alas, the fox is out of luck.\"\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9c1731c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The quick brown fox jumps over the lazy dog However the dog doesnt seem impressed Oh no it just yawned How disappointing Maybe a squirrel would elicit a reaction Alas the fox is out of luck'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_punc(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53863df6",
   "metadata": {},
   "source": [
    "**Hence the function removes the punctuations from the text and we can also use this function to remove the punctuations from the corpus.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f062b9df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@dananner night, darlin'!  sweet dreams to you \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'dananner night darlin  sweet dreams to you '"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exmaple on whole Dataset.\n",
    "print(df['review'][9])\n",
    "\n",
    "# Remove Punctuation\n",
    "remove_punc(df['review'][9])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11df4d1a",
   "metadata": {},
   "source": [
    "## 5. Handling ChatWords\n",
    "\n",
    "¬∂\n",
    "Handling ChatWords, also known as internet slang or informal language used in online communication, is important in NLP text preprocessing to ensure accurate analysis and understanding of text data. By converting ChatWords into their standard English equivalents or formal language equivalents, NLP models can effectively interpret the meaning of the text. This preprocessing step helps in maintaining consistency, improving the quality of input data, and enhancing the performance of NLP tasks such as sentiment analysis, chatbots, and information retrieval systems. Ultimately, handling ChatWords ensures better comprehension and more reliable results in NLP applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "af4da18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here Come ChatWords Which i Get from a Github Repository\n",
    "# Repository Link : https://github.com/rishabhverma17/sms_slang_translator/blob/master/slang.txt\n",
    "chat_words = {\n",
    "    \"AFAIK\": \"As Far As I Know\",\n",
    "    \"AFK\": \"Away From Keyboard\",\n",
    "    \"ASAP\": \"As Soon As Possible\",\n",
    "    \"ATK\": \"At The Keyboard\",\n",
    "    \"ATM\": \"At The Moment\",\n",
    "    \"A3\": \"Anytime, Anywhere, Anyplace\",\n",
    "    \"BAK\": \"Back At Keyboard\",\n",
    "    \"BBL\": \"Be Back Later\",\n",
    "    \"BBS\": \"Be Back Soon\",\n",
    "    \"BFN\": \"Bye For Now\",\n",
    "    \"B4N\": \"Bye For Now\",\n",
    "    \"BRB\": \"Be Right Back\",\n",
    "    \"BRT\": \"Be Right There\",\n",
    "    \"BTW\": \"By The Way\",\n",
    "    \"B4\": \"Before\",\n",
    "    \"B4N\": \"Bye For Now\",\n",
    "    \"CU\": \"See You\",\n",
    "    \"CUL8R\": \"See You Later\",\n",
    "    \"CYA\": \"See You\",\n",
    "    \"FAQ\": \"Frequently Asked Questions\",\n",
    "    \"FC\": \"Fingers Crossed\",\n",
    "    \"FWIW\": \"For What It's Worth\",\n",
    "    \"FYI\": \"For Your Information\",\n",
    "    \"GAL\": \"Get A Life\",\n",
    "    \"GG\": \"Good Game\",\n",
    "    \"GN\": \"Good Night\",\n",
    "    \"GMTA\": \"Great Minds Think Alike\",\n",
    "    \"GR8\": \"Great!\",\n",
    "    \"G9\": \"Genius\",\n",
    "    \"IC\": \"I See\",\n",
    "    \"ICQ\": \"I Seek you (also a chat program)\",\n",
    "    \"ILU\": \"ILU: I Love You\",\n",
    "    \"IMHO\": \"In My Honest/Humble Opinion\",\n",
    "    \"IMO\": \"In My Opinion\",\n",
    "    \"IOW\": \"In Other Words\",\n",
    "    \"IRL\": \"In Real Life\",\n",
    "    \"KISS\": \"Keep It Simple, Stupid\",\n",
    "    \"LDR\": \"Long Distance Relationship\",\n",
    "    \"LMAO\": \"Laugh My A.. Off\",\n",
    "    \"LOL\": \"Laughing Out Loud\",\n",
    "    \"LTNS\": \"Long Time No See\",\n",
    "    \"L8R\": \"Later\",\n",
    "    \"MTE\": \"My Thoughts Exactly\",\n",
    "    \"M8\": \"Mate\",\n",
    "    \"NRN\": \"No Reply Necessary\",\n",
    "    \"OIC\": \"Oh I See\",\n",
    "    \"PITA\": \"Pain In The A..\",\n",
    "    \"PRT\": \"Party\",\n",
    "    \"PRW\": \"Parents Are Watching\",\n",
    "    \"QPSA?\": \"Que Pasa?\",\n",
    "    \"ROFL\": \"Rolling On The Floor Laughing\",\n",
    "    \"ROFLOL\": \"Rolling On The Floor Laughing Out Loud\",\n",
    "    \"ROTFLMAO\": \"Rolling On The Floor Laughing My A.. Off\",\n",
    "    \"SK8\": \"Skate\",\n",
    "    \"STATS\": \"Your sex and age\",\n",
    "    \"ASL\": \"Age, Sex, Location\",\n",
    "    \"THX\": \"Thank You\",\n",
    "    \"TTFN\": \"Ta-Ta For Now!\",\n",
    "    \"TTYL\": \"Talk To You Later\",\n",
    "    \"U\": \"You\",\n",
    "    \"U2\": \"You Too\",\n",
    "    \"U4E\": \"Yours For Ever\",\n",
    "    \"WB\": \"Welcome Back\",\n",
    "    \"WTF\": \"What The F...\",\n",
    "    \"WTG\": \"Way To Go!\",\n",
    "    \"WUF\": \"Where Are You From?\",\n",
    "    \"W8\": \"Wait...\",\n",
    "    \"7K\": \"Sick:-D Laugher\",\n",
    "    \"TFW\": \"That feeling when\",\n",
    "    \"MFW\": \"My face when\",\n",
    "    \"MRW\": \"My reaction when\",\n",
    "    \"IFYP\": \"I feel your pain\",\n",
    "    \"TNTL\": \"Trying not to laugh\",\n",
    "    \"JK\": \"Just kidding\",\n",
    "    \"IDC\": \"I don't care\",\n",
    "    \"ILY\": \"I love you\",\n",
    "    \"IMU\": \"I miss you\",\n",
    "    \"ADIH\": \"Another day in hell\",\n",
    "    \"ZZZ\": \"Sleeping, bored, tired\",\n",
    "    \"WYWH\": \"Wish you were here\",\n",
    "    \"TIME\": \"Tears in my eyes\",\n",
    "    \"BAE\": \"Before anyone else\",\n",
    "    \"FIMH\": \"Forever in my heart\",\n",
    "    \"BSAAW\": \"Big smile and a wink\",\n",
    "    \"BWL\": \"Bursting with laughter\",\n",
    "    \"BFF\": \"Best friends forever\",\n",
    "    \"CSL\": \"Can't stop laughing\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fdcc33b",
   "metadata": {},
   "source": [
    "The code defines a function, chat_conversion, that replaces text with their corresponding chat acronyms from a predefined dictionary. It iterates through each word in the input text, checks if it exists in the dictionary, and replaces it if found. The modified text is then returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "24ada507",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_conversion(text): \n",
    "    new_text = []\n",
    "    for i in text.split(): \n",
    "        if i.upper() in chat_words: \n",
    "            new_text.append(chat_words[i.upper()])\n",
    "        else: \n",
    "            new_text.append(i)\n",
    "    return \" \".join(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7a5d80bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In My Honest/Humble Opinion he is the best\n",
      "For Your Information New Delhi is the capital of India\n"
     ]
    }
   ],
   "source": [
    "# Text\n",
    "text = 'IMHO he is the best'\n",
    "text1 = 'FYI New Delhi is the capital of India'\n",
    "# Calling function\n",
    "print(chat_conversion(text))\n",
    "print(chat_conversion(text1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264492aa",
   "metadata": {},
   "source": [
    "## 6. Spelling Correction\n",
    "\n",
    "¬∂\n",
    "Spelling correction is a crucial aspect of NLP text preprocessing to enhance data quality and improve model performance. It addresses errors in text caused by typographical mistakes, irregularities, or variations in spelling. Correcting spelling errors ensures consistency and accuracy in the dataset, reducing ambiguity and improving the reliability of NLP tasks like sentiment analysis, machine translation, and information retrieval. By standardizing spelling across the dataset, models can better understand and process text, leading to more precise and reliable results in natural language processing applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "44d681a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import this Library to Handle the Spelling Issue.\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f1155918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ceertain conditionas duriing seveal ggenerations aree moodified in the saame maner.\n",
      "certain conditions during several generations are modified in the same manner.\n",
      "The cat sat on the cuchion. while plyaiing\n",
      "The cat sat on the cushion. while playing\n"
     ]
    }
   ],
   "source": [
    "# Incorrect text\n",
    "incorrect_text = 'ceertain conditionas duriing seveal ggenerations aree moodified in the saame maner.'\n",
    "print(incorrect_text)\n",
    "# Text 2 \n",
    "incorrect_text2 = 'The cat sat on the cuchion. while plyaiing'\n",
    "# Calling function\n",
    "textBlb = TextBlob(incorrect_text)\n",
    "textBlb1 = TextBlob(incorrect_text2)\n",
    "# Corrected Text\n",
    "print(textBlb.correct().string)\n",
    "print(incorrect_text2)\n",
    "print(textBlb1.correct().string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65c2e09",
   "metadata": {},
   "source": [
    "## 7. Handling StopWords\n",
    "\n",
    "¬∂\n",
    "In NLP text preprocessing, removing stop words is crucial to enhance the quality and efficiency of analysis. Stop words are common words like \"the,\" \"is,\" and \"and,\" which appear frequently in text but carry little semantic meaning. By eliminating stop words, we reduce noise in the data, decrease the dimensionality of the dataset, and improve the accuracy of NLP tasks such as sentiment analysis, topic modeling, and text classification. This process streamlines the analysis by focusing on the significant words that carry more meaningful information, leading to better model performance and interpretation of results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f5b1db1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use NLTK library to remove Stopwords.\n",
    "from nltk.corpus import stopwords\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6ad85903",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/sanjaymahto/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download stopwords (only needs to be run once)\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "809cf5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we can see all the stopwords in English.However we can chose different Languages also like spanish etc.\n",
    "stopword = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cabc24c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function\n",
    "def remove_stopwords(text):\n",
    "    new_text = []\n",
    "    \n",
    "    for word in text.split():\n",
    "        if word in stopword:\n",
    "            new_text.append('')\n",
    "        else:\n",
    "            new_text.append(word)\n",
    "    x = new_text[:]\n",
    "    new_text.clear()\n",
    "    return \" \".join(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4acf2c1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text With Stop Words :probably my all-time favorite movie, a story of selflessness, sacrifice and dedication to a noble cause, but it's not preachy or boring. it just never gets old, despite my having seen it some 15 or more times\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'probably  all-time favorite movie,  story  selflessness, sacrifice  dedication   noble cause,    preachy  boring.   never gets old, despite   seen   15   times'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Text\n",
    "text = 'probably my all-time favorite movie, a story of selflessness, sacrifice and dedication to a noble cause, but it\\'s not preachy or boring. it just never gets old, despite my having seen it some 15 or more times'\n",
    "print(f'Text With Stop Words :{text}')\n",
    "# Calling Function\n",
    "remove_stopwords(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "002ab103",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                  real good moment.  missssssssss   much,\n",
       "1                   reading manga http://plurk.com/p/mzp1e\n",
       "2        @comeagainjen http://twitpic.com/2y2lx - http:...\n",
       "3        @lapcat need  send 'em   accountant tomorrow. ...\n",
       "4                 add   myspace!!! myspace.com/lookthunder\n",
       "                               ...                        \n",
       "10309     depression  g herbo   mood   on,  done stress...\n",
       "10310         depression succumbs  brain  makes  feel l...\n",
       "10311    ketamine nasal spray shows promise  depression...\n",
       "10312    dont mistake  bad day  depression! everyone  'em!\n",
       "10313                                                    0\n",
       "Name: review, Length: 10314, dtype: object"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can Apply the same Function on Whole Corpus also \n",
    "df['review'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff45b918",
   "metadata": {},
   "source": [
    "## 8. Handling Emojies\n",
    "\n",
    "¬∂\n",
    "Handling emojis in NLP text preprocessing is essential for several reasons. Emojis convey valuable information about sentiment, emotion, and context in text data, especially in informal communication channels like social media. However, they pose challenges for NLP algorithms due to their non-textual nature. Preprocessing involves converting emojis into meaningful representations, such as replacing them with textual descriptions or mapping them to specific sentiment categories. By handling emojis effectively, NLP models can accurately interpret and analyze text data, leading to improved performance in sentiment analysis, emotion detection, and other NLP tasks.\n",
    "\n",
    "##### 8.1 Simply Remove Emojis\n",
    "The code defines a function, remove_emoji, which uses a regular expression to match and remove all emojis from a given text string. It targets various Unicode ranges corresponding to different categories of emojis and replaces them with an empty string, effectively removing them from the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1a67dcbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again Here we use The Regular Expressions to Remove the Emojies from Text or Whole Corpus.\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0a362480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loved the movie. It was üòò \n",
      " Python is üî• \n",
      " üòÖü•≤‚úçüèªüòéHehehehehe\n",
      "Loved the movie. It was \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ü•≤Hehehehehe'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Texts \n",
    "text = \"Loved the movie. It was üòò\"\n",
    "text1 = 'Python is üî•'\n",
    "kuch = \"üòÖü•≤‚úçüèªüòéHehehehehe\"\n",
    "print(text ,'\\n', text1, '\\n', kuch)\n",
    "\n",
    "# Remove Emojies using Fucntion\n",
    "print(remove_emoji(text))\n",
    "remove_emoji(text1)\n",
    "remove_emoji(kuch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560c9408",
   "metadata": {},
   "source": [
    "Well the fucntion is removing the emojies easily.\n",
    "\n",
    "**8.2 Simply Convert Emojis into text¬∂**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5ed920ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will USe the Emoji Libray to handle this task \n",
    "# Pip Install emoji\n",
    "import emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ac366b1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loved the movie. It was :face_blowing_a_kiss:\n",
      "Python is :fire:\n",
      ":grinning_face_with_sweat::smiling_face_with_tear::writing_hand_light_skin_tone::smiling_face_with_sunglasses:Hehehehehe\n"
     ]
    }
   ],
   "source": [
    "# Calling the Emoji tool Demojize.\n",
    "print(emoji.demojize(text))\n",
    "print(emoji.demojize(text1))\n",
    "print(emoji.demojize(kuch))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6ecd24",
   "metadata": {},
   "source": [
    "## 9. Tokenization\n",
    "\n",
    "¬∂\n",
    "Tokenization is a crucial step in NLP text preprocessing where text is segmented into smaller units, typically words or subwords, known as tokens. This process is essential for several reasons. Firstly, it breaks down the text into manageable units for analysis and processing. Secondly, it standardizes the representation of words, enabling consistency in language modeling tasks. Additionally, tokenization forms the basis for feature extraction and modeling in NLP, facilitating tasks such as sentiment analysis, named entity recognition, and machine translation. Overall, tokenization plays a fundamental role in preparing text data for further analysis and modeling in NLP applications.\n",
    "\n",
    "We Generally do 2 Type of tokenization 1. Word tokenization 2. Sentence Tokenization\n",
    "\n",
    "**9.1 NLTK**\n",
    "\n",
    "NLTK is a Library used to tokenize text into sentences and words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bab5e553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraray \n",
    "from nltk.tokenize import word_tokenize,sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "90f4b3be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/sanjaymahto/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4e01c8ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'going', 'to', 'visit', 'delhi', '!']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Text\n",
    "sentence = 'I am going to visit delhi!'\n",
    "# Calling tool\n",
    "word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435b103c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I am going to visit delhi!']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "sent_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fcc0949f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Lorem Ipsum is simply dummy text of the printing and typesetting industry?',\n",
       " \"Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, \\nwhen an unknown printer took a galley of type and scrambled it to make a type specimen book.\"]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Whole text Containing 2 or more Sentences\n",
    "text = \"\"\"Lorem Ipsum is simply dummy text of the printing and typesetting industry? \n",
    "Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, \n",
    "when an unknown printer took a galley of type and scrambled it to make a type specimen book.\"\"\"\n",
    "\n",
    "# Sentence Based Tokenization\n",
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "50b55d77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'have', 'a', 'Ph.D', 'in', 'A.I']\n",
      "['We', \"'re\", 'here', 'to', 'help', '!', 'mail', 'us', 'at', 'nks', '@', 'gmail.com']\n",
      "['A', '5km', 'ride', 'cost', '$', '10.50']\n"
     ]
    }
   ],
   "source": [
    "# Some Sentences \n",
    "sent5 = 'I have a Ph.D in A.I'\n",
    "sent6 = \"We're here to help! mail us at nks@gmail.com\"\n",
    "sent7 = 'A 5km ride cost $10.50'\n",
    "\n",
    "# Word Tokenize the Sentences\n",
    "print(word_tokenize(sent5))\n",
    "print(word_tokenize(sent6))\n",
    "print(word_tokenize(sent7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cec72b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
